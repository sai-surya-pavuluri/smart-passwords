{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":248720,"sourceType":"datasetVersion","datasetId":104631}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-31T18:07:24.534004Z","iopub.execute_input":"2025-08-31T18:07:24.534223Z","iopub.status.idle":"2025-08-31T18:07:25.943356Z","shell.execute_reply.started":"2025-08-31T18:07:24.534197Z","shell.execute_reply":"2025-08-31T18:07:25.942565Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/common-password-list-rockyoutxt/rockyou.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!cp /kaggle/input/common-password-list-rockyoutxt/rockyou.txt /kaggle/working/passwords.txt\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T18:08:27.276380Z","iopub.execute_input":"2025-08-31T18:08:27.276636Z","iopub.status.idle":"2025-08-31T18:08:29.066158Z","shell.execute_reply.started":"2025-08-31T18:08:27.276614Z","shell.execute_reply":"2025-08-31T18:08:29.065382Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Load passwords\nwith open(\"/kaggle/working/passwords.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n    passwords = [line.strip() for line in f if line.strip()]  # remove blank lines","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T18:08:34.500981Z","iopub.execute_input":"2025-08-31T18:08:34.501239Z","iopub.status.idle":"2025-08-31T18:08:37.084797Z","shell.execute_reply.started":"2025-08-31T18:08:34.501214Z","shell.execute_reply":"2025-08-31T18:08:37.084224Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Filter passwords by length\nmin_len = 4\nmax_len = 30\n\nfiltered_passwords = [p for p in passwords if len(p) >= min_len and len(p) <= max_len]\n\nprint(f\"After filtering: {len(filtered_passwords)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T18:08:41.820732Z","iopub.execute_input":"2025-08-31T18:08:41.821372Z","iopub.status.idle":"2025-08-31T18:08:42.717649Z","shell.execute_reply.started":"2025-08-31T18:08:41.821342Z","shell.execute_reply":"2025-08-31T18:08:42.716873Z"}},"outputs":[{"name":"stdout","text":"After filtering: 14338521\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Filter for unique passwords\nseen = set()\nunique_passwords = []\nfor password in filtered_passwords:\n    if password not in seen:\n        unique_passwords.append(password)\n        seen.add(password)\nprint(f\"After filtering: {len(unique_passwords)}\")\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T18:11:41.063805Z","iopub.execute_input":"2025-08-31T18:11:41.064093Z","iopub.status.idle":"2025-08-31T18:11:47.149342Z","shell.execute_reply.started":"2025-08-31T18:11:41.064072Z","shell.execute_reply":"2025-08-31T18:11:47.148511Z"}},"outputs":[{"name":"stdout","text":"After filtering: 14337861\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Create the vocabulary\nvocab = set()\nfor password in unique_passwords:\n    for ch in password:\n        if ch not in vocab:\n            vocab.add(ch)\n\nprint(f\"Vocabulary: {vocab}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T18:12:57.807855Z","iopub.execute_input":"2025-08-31T18:12:57.808121Z","iopub.status.idle":"2025-08-31T18:13:06.821588Z","shell.execute_reply.started":"2025-08-31T18:12:57.808095Z","shell.execute_reply":"2025-08-31T18:13:06.820818Z"}},"outputs":[{"name":"stdout","text":"Vocabulary: {'0', 'ζ', 'း', 'ྲ', '๊', '£', 'К', 'Ü', 'τ', '☼', 'บ', 'н', 'ƒ', 'に', 'γ', '�', 'ศ', 'G', 'ｉ', 'ย', 'ğ', '*', 'á', 'ו', 'i', '‗', '—', 'დ', '¤', 'ว', 'õ', 'Ě', ',', 'Æ', '╩', 'ｗ', 'ם', 'ф', '”', 'გ', '荐', '๛', 'ๆ', 'ы', 'ห', 'ถ', 'མ', 'ེ', '•', 'q', 'ო', '႑', 'I', 'ๅ', 'ေ', 'โ', 'خ', '❤', 'ấ', 'ز', 'ג', 'ข', '၆', '∂', 'Œ', 'π', 'Ś', 'ת', 'ช', 'ئ', 'Å', '`', 'J', 'စ', 'и', 'ﾋ', 'Σ', 'ﾁ', '½', 'D', 'ó', 'س', 'ð', 'น', 'ป', 'ه', 'ฃ', '¿', 'က', 'T', '4', '–', '♠', 'ק', 'к', 'ن', 'L', 'ö', 'ల', '¯', 'ر', 'U', '/', '๔', 'И', '๖', 'ư', '（', 'Κ', 'ぽ', 'ؘ', 'ľ', 'Θ', 'ל', 'ح', 'ฌ', 'ㄓ', '\\x03', 'Ο', 'ึ', 'ㅐ', 'у', 'ภ', 'P', 'э', 'Д', 'ء', 'ś', 'ะ', '¡', 'ｼ', 'Ҋ', 'ｲ', 'ت', 'с', 'û', 'l', '\"', 'ნ', 'ူ', 'ฦ', 'ي', 'А', 'ｿ', 'ù', 'Õ', 'బ', '）', 'ა', 'Ċ', 'إ', 'ไ', 'จ', '我', '孵', 'ည', '·', 'క', 'ř', '底', 'ㅋ', 'Л', 'Τ', '¦', 'µ', 'י', 'ِ', 'Ã', '†', '【', 'ב', 'ㄴ', '๓', 'ى', '«', '７', 'ฯ', 'm', 'ç', 'ץ', '=', '»', 'נ', '̉', '｡', 'า', 'Υ', 'ل', '÷', 'Я', 'ү', 'อ', 'พ', '¬', 'ฉ', 'ㅕ', 'β', 'ч', 'R', 'ä', 'מ', 'ш', 'Ā', '9', '™', 'V', '@', 'ą', 'ξ', 'ｱ', 'м', 'တ', 'j', 'ｎ', 'د', 'ｘ', '６', 'v', '_', '…', '×', 'Ş', '\\u200e', 'b', '်', '๋', 'ÿ', '宆', 'e', 'ث', 'ü', '2', 'ј', 'כ', '日', 'ć', 'E', '¢', '₧', 'ด', '1', 'ㄷ', 'N', 'え', '་', 'ט', 'k', 'п', 'အ', 'Γ', '็', 'ơ', '№', 'æ', '฿', 'ฐ', 'గ', 'ה', '\\xa0', 'ん', 'Ò', 'ပ', 'ㅊ', ';', '０', 'Ī', 'Č', '\\x8f', '\\x7f', 'ㅣ', '¥', '▒', 'Π', '؟', '䬳', '寯', '\\x1a', 'ș', 'H', ' ', 'ị', 'ε', 'Ë', 'ë', 'г', 'ỳ', '●', 'ý', 'ő', '３', '‘', '²', 'Ổ', 'ø', 'р', 'ż', '１', 'غ', 'ח', '东', '墾', 'ž', 'O', '#', 'ﾖ', '西', 'Đ', 'آ', '©', 'ª', 'ź', '█', '(', 'Χ', '♥', '့', 'л', 'ﾓ', 'à', 'န', '‰', 'ν', '♫', 'Ρ', 'ר', 'Ϊ', 'þ', 'ฆ', 'ﾜ', 'ĩ', '±', 'å', 'ၾ', '̀', 'ﾉ', '˓', 'я', 'ă', 'g', '3', 'З', '^', '8', 'A', 'č', 'べ', 'א', 'щ', 'φ', 'Η', 'ณ', 'Ø', '์', 'w', 'đ', 'Г', '◕', 'ฝ', '唔', 'À', 'Ô', 'F', 'ι', 'ף', 'ｺ', 'Ç', '&', 'ك', '○', '☜', 'K', 'h', 'づ', ')', 'เ', 'Ä', 'ｐ', 'Ч', '♦', '€', '♂', 'n', '®', 'ן', 'ิ', 'ı', 'သ', 'š', 'こ', 'ş', 'η', 'أ', 'ค', '“', 'ע', 'ì', '่', 'ж', 'ю', 'a', 'ธ', 'ฎ', 'ม', '到', '$', 'Ι', 'Ł', '˿', 'ً', 'ท', 'ฤ', 'ထ', 'ә', 'ㅠ', 'ก', 'в', 'Ь', 'є', '\\x9d', 'ß', '５', 'ѕ', 'Λ', '°', 'ổ', 'z', 'ς', 'ｾ', 'ը', 'ฮ', 'ూ', 'θ', 'ج', 'ี', '๐', '،', '╨', 'ů', '\\\\', '使', '้', 'è', 'ต', 'ი', ']', 'œ', '９', 'Ф', '★', 'W', '♣', 'ე', '!', 'ယ', 'д', '́', 'ã', 'ི', '\\x08', 'X', 'ˆ', 'ū', 'б', '☞', 'u', 'ს', ':', 'م', 'ẻ', '๗', '7', 'ب', 'ﾄ', 'ь', 'ę', 'Μ', 'C', 'Ó', '샤', 'º', 'ฺ', '๘', 'ف', 'ད', '๑', '๕', 'ω', '[', 'ך', 'แ', '>', 'y', 'ع', '】', 'ě', 'Z', 'ọ', 'μ', '{', '¨', '~', '☻', 'ั', 'ố', 'ื', 'Α', '.', 'f', 'o', 'ъ', 'ใ', 'ؤ', 'Ê', 'σ', 'ち', 'ခ', \"'\", 't', 'r', '∆', 'Ö', 'ú', 'ț', 'ا', '番', 'ﾏ', '\\u200f', 'ű', '³', '\\x04', 'ﾇ', 'S', 'ร', 'و', 'ớ', '你', 'ာ', 'ฬ', 'й', '5', 'ẩ', 'κ', 'ҹ', 'Ν', '§', 'ผ', '¸', '-', 'ﾐ', '္', 'Ñ', 'ï', '?', '８', '空', 'ť', 'Ѽ', 'ｈ', '◙', 'е', '%', 'é', 'ｴ', 'џ', 'ψ', 'ỗ', 'B', '存', 'ỏ', '‿', 'ရ', 'ซ', '\\U00065bee', '͓', 'x', 'ฟ', 's', '保', 'ê', 'λ', 'ẹ', 'ד', 'c', 'Н', 'â', 'Ε', 'd', 'ѹ', 'î', 'ส', '聽', 'ﾗ', '推', 'Y', 'í', 'ц', 'ס', 'ฏ', '´', '牙', 'х', 'ำ', 'მ', 'İ', 'а', 'ق', 'ش', '|', '¶', 'ﾝ', '۶', '年', 'Â', 'ో', 'о', 'Ì', '+', 'Ğ', 'ђ', 'צ', 'ז', 'Ï', '\\U0006f862', 'ﾅ', '▬', 'Í', 'ł', '的', 'ฑ', 'ํ', 'É', 'ﾘ', 'ㅁ', 'Q', '¹', 'ò', 'ู', 'Е', 'Î', 'ο', 'ｵ', 'ô', 'υ', 'Á', '๙', 'ง', 'ی', 'δ', 'פ', '▼', '<', 'Ž', 'Ÿ', 'ض', 'з', '}', 'α', 'ρ', 'ล', 'т', 'Ы', '↓', 'ظ', '箮', 'ุ', 'ص', 'ñ', 'ط', 'p', 'ษ', 'は', 'Š', 'ة', 'χ', '╥', 'ẽ', '6', '။', 'Ř', 'ﾕ', 'ฒ', '̃', 'M', '２', '๒', 'Ц', 'ญ', '’', 'ש', 'င'}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import string\n\n# Allowed characters (printable English keyboard)\nallowed = set(string.printable) - set(string.whitespace)  # drop tabs, newlines, etc.\nallowed.add(\" \")  # keep space explicitly\n\n# English passwords\nenglish_passwords = [p for p in unique_passwords if all(ch in allowed for ch in p)]\n\nprint(f\"Original: {len(unique_passwords)}, English: {len(english_passwords)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T18:17:54.779854Z","iopub.execute_input":"2025-08-31T18:17:54.780397Z","iopub.status.idle":"2025-08-31T18:18:03.185633Z","shell.execute_reply.started":"2025-08-31T18:17:54.780372Z","shell.execute_reply":"2025-08-31T18:18:03.184877Z"}},"outputs":[{"name":"stdout","text":"Original: 14337861, English: 14323646\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Filter for only ASCII characters\nvocab = set()\nfor password in english_passwords:\n    for ch in password:\n        if ch not in vocab:\n            vocab.add(ch)\n\nprint(f\"Size of vocabulary: {(vocab)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T18:18:43.422914Z","iopub.execute_input":"2025-08-31T18:18:43.423422Z","iopub.status.idle":"2025-08-31T18:18:52.547794Z","shell.execute_reply.started":"2025-08-31T18:18:43.423400Z","shell.execute_reply":"2025-08-31T18:18:52.547148Z"}},"outputs":[{"name":"stdout","text":"Size of vocabulary: {'g', '0', '=', '3', '^', '8', 'A', 'S', 'w', '5', 'G', 'R', '*', '-', 'i', '?', 'F', '&', '9', ',', 'K', 'h', ')', 'V', '@', '%', 'n', 'B', 'j', 'x', 's', 'q', 'v', '_', 'I', 'c', 'b', 'd', 'a', '$', 'e', '2', 'E', 'Y', '`', 'J', '1', 'N', 'z', 'k', 'D', '|', '\\\\', 'T', '4', '+', ']', ';', 'W', 'L', '!', 'U', '/', 'Q', 'X', 'u', ':', '7', 'H', ' ', 'C', 'P', '[', '<', '>', 'l', 'y', '}', 'Z', '\"', '{', 'O', '#', '~', '(', 'p', '.', 'f', 'o', '6', 'M', \"'\", 't', 'r', 'm'}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Sort charcters in vocabulary\nvocabulary = list(vocab)\nvocabulary.sort()\nvocabulary.insert(0,'<EOS>')\nvocabulary.append('</EOS>')\nvocabulary.append('<PAD>')\n\n# Create maps to match character with associated indices\nch_to_idx = {ch:i for i,ch in enumerate(vocabulary)}\nidx_to_chx = {i : ch for i,ch in enumerate(vocabulary)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T18:47:47.310688Z","iopub.execute_input":"2025-08-31T18:47:47.310971Z","iopub.status.idle":"2025-08-31T18:47:47.315697Z","shell.execute_reply.started":"2025-08-31T18:47:47.310950Z","shell.execute_reply":"2025-08-31T18:47:47.314929Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Create two parallel lists of inputs and associated targets for each password to train an RNN\ninputs = []\ntargets = []\nfor password in english_passwords:\n    inp = [0]\n    target = []\n    for ch in password:\n        inp.append(ch_to_idx[ch])\n        target.append(ch_to_idx[ch])\n    inputs.append(inp)\n    target.append(96)\n    targets.append(target)\nexamples = list(zip(inputs, targets))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T18:47:51.259651Z","iopub.execute_input":"2025-08-31T18:47:51.259899Z","iopub.status.idle":"2025-08-31T18:48:52.727015Z","shell.execute_reply.started":"2025-08-31T18:47:51.259882Z","shell.execute_reply":"2025-08-31T18:48:52.726454Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"PAD_ID = 97\n\ndef make_batch(examples):\n    \"\"\"\n    examples: list of (inp, tgt) where each is a list[int]\n              inp starts with <s>, tgt ends with </s>, and len(inp) == len(tgt) before padding.\n\n    returns:\n      padded_inps: np.ndarray [B, L_max] int\n      padded_tgts: np.ndarray [B, L_max] int\n      mask:        np.ndarray [B, L_max] float32 (1.0 for real target positions, 0.0 for PAD)\n    \"\"\"\n    # 1) Find max length for this batch\n    max_len = 0\n    for inp, _ in examples:\n        if len(inp) > max_len:\n            max_len = len(inp)\n\n    B = len(examples)\n    padded_inps = np.full((B, max_len), PAD_ID, dtype=np.int32)\n    padded_tgts = np.full((B, max_len), PAD_ID, dtype=np.int32)\n    mask        = np.zeros((B, max_len), dtype=np.float32)  # 1 for real target tokens, 0 for PAD\n\n    # 2) Right-pad and build mask using original (pre-pad) length\n    for idx, (inp, tgt) in enumerate(examples):\n        real_len = len(tgt)           # same as len(inp) pre-padding; mask is defined over targets\n        padded_inps[idx, :real_len] = inp\n        padded_tgts[idx, :real_len] = tgt\n        mask[idx, :real_len] = 1.0    # END_ID is included in real_len, pads are 0.0\n\n    return padded_inps, padded_tgts, mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T18:49:11.353421Z","iopub.execute_input":"2025-08-31T18:49:11.353967Z","iopub.status.idle":"2025-08-31T18:49:11.359490Z","shell.execute_reply.started":"2025-08-31T18:49:11.353946Z","shell.execute_reply":"2025-08-31T18:49:11.358858Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import random\nrandom.seed(42)\nrandom.shuffle(examples)\n\nn = len(examples)\ntrain_end = int(0.8 * n)\nval_end   = int(0.9 * n)\n\ntrain_examples = examples[:100000]\nval_examples   = examples[100000:110000]\ntest_examples  = examples[110000:120000]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T19:05:04.133272Z","iopub.execute_input":"2025-08-31T19:05:04.134048Z","iopub.status.idle":"2025-08-31T19:05:13.338621Z","shell.execute_reply.started":"2025-08-31T19:05:04.134026Z","shell.execute_reply":"2025-08-31T19:05:13.337794Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# ====== CONFIG ======\nimport numpy as np\nimport tensorflow as tf\n\nSTART_ID = 0\nEND_ID = 96\nPAD_ID = 97\n\n# - vocab_size: int (including <s>, </s>, <PAD>, all chars)\n# - train_examples: list[tuple[list[int], list[int]]]  # (inp, tgt) per password\n# - val_examples:   list[tuple[list[int], list[int]]]\n# - make_batch(examples) -> (padded_inps, padded_tgts, mask) exactly like you wrote\n\nvocab_size = len(vocabulary)\nmodel_config = {\n    'embedding_dim': 64,\n    'hidden_size': 256,\n    'dropout': 0.2,\n    'grad_clip': 1.0,\n    'batch_size': 256,\n    'lr': 1e-3\n}\n\n\n\n# ====== MODEL ======\ndef build_model(vocab_size: int, cfg: dict) -> tf.keras.Model:\n    inp_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"inp\")\n    x = tf.keras.layers.Embedding(\n        input_dim=vocab_size,\n        output_dim=cfg['embedding_dim'],\n        mask_zero=False  # we provide our own explicit loss mask\n    )(inp_ids)\n    x = tf.keras.layers.GRU(\n        cfg['hidden_size'],\n        return_sequences=True,\n        dropout=cfg['dropout']\n    )(x)\n    logits = tf.keras.layers.Dense(vocab_size)(x)  # [B, L, V]\n    return tf.keras.Model(inputs=inp_ids, outputs=logits)\n\nmodel = build_model(vocab_size, model_config)\n\noptimizer = tf.keras.optimizers.Adam(\n    learning_rate=model_config['lr'],\n    clipnorm=model_config['grad_clip']  # gradient clipping\n)\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n\n# ====== ONE TRAIN STEP (masked) ======\n@tf.function\ndef train_step(padded_inps, padded_tgts, mask):\n    with tf.GradientTape() as tape:\n        logits = model(padded_inps, training=True)          # [B, L, V]\n        per_tok_loss = loss_fn(padded_tgts, logits)         # [B, L]\n        masked_loss = per_tok_loss * mask                   # zero out PADs\n        loss = tf.reduce_sum(masked_loss) / tf.reduce_sum(mask)  # avg over real tokens\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    return loss\n\n@tf.function\ndef val_step(padded_inps, padded_tgts, mask):\n    logits = model(padded_inps, training=False)\n    per_tok_loss = loss_fn(padded_tgts, logits)\n    masked_loss = per_tok_loss * mask\n    loss = tf.reduce_sum(masked_loss) / tf.reduce_sum(mask)\n    return loss\n\n# ====== EPOCH LOOP ======\ndef run_epoch(examples, batch_size, train=True):\n    # (Optional) bucket by length first to reduce padding; simple shuffle here:\n    np.random.shuffle(examples)\n    total_loss = 0.0\n    total_tokens = 0.0\n\n    for i in range(0, len(examples), batch_size):\n        batch = examples[i:i+batch_size]\n        padded_inps, padded_tgts, mask = make_batch(batch)   # your function\n        # to tensors\n        padded_inps = tf.convert_to_tensor(padded_inps, dtype=tf.int32)\n        padded_tgts = tf.convert_to_tensor(padded_tgts, dtype=tf.int32)\n        mask_t      = tf.convert_to_tensor(mask, dtype=tf.float32)\n\n        if train:\n            loss = train_step(padded_inps, padded_tgts, mask_t)\n        else:\n            loss = val_step(padded_inps, padded_tgts, mask_t)\n\n        # accumulate by real-token count to get a stable epoch average\n        real_tokens = float(np.sum(mask))\n        total_loss += float(loss) * real_tokens\n        total_tokens += real_tokens\n\n    return total_loss / max(total_tokens, 1.0)\n\n# ====== TRAINING SCHEDULE ======\nEPOCHS = 100\nbest_val = float('inf')\npatience = 3\nstale = 0\n\nfor epoch in range(1, EPOCHS+1):\n    train_loss = run_epoch(train_examples, model_config['batch_size'], train=True)\n    val_loss   = run_epoch(val_examples,   model_config['batch_size'], train=False)\n\n    print(f\"Epoch {epoch:02d} | train_loss {train_loss:.4f} | val_loss {val_loss:.4f}\")\n\n    # checkpoint best\n    if val_loss < best_val - 1e-4:\n        best_val = val_loss\n        stale = 0\n        model.save_weights(\"best_rnn.weights.h5\")\n    else:\n        stale += 1\n        # reduce LR on plateau\n        if stale >= 2:\n            old = float(optimizer.learning_rate.numpy())\n            optimizer.learning_rate.assign(old * 0.5)\n            print(f\"Reduced LR: {old:.2e} -> {float(optimizer.learning_rate.numpy()):.2e}\")\n        if stale >= patience:\n            print(\"Early stopping.\")\n            break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T19:11:09.511968Z","iopub.execute_input":"2025-08-31T19:11:09.512337Z","iopub.status.idle":"2025-08-31T19:13:02.578250Z","shell.execute_reply.started":"2025-08-31T19:11:09.512289Z","shell.execute_reply":"2025-08-31T19:13:02.577479Z"}},"outputs":[{"name":"stdout","text":"Epoch 01 | train_loss 3.0355 | val_loss 2.8848\nEpoch 02 | train_loss 2.8495 | val_loss 2.8396\nEpoch 03 | train_loss 2.8082 | val_loss 2.8061\nEpoch 04 | train_loss 2.7800 | val_loss 2.7845\nEpoch 05 | train_loss 2.7585 | val_loss 2.7647\nEpoch 06 | train_loss 2.7390 | val_loss 2.7522\nEpoch 07 | train_loss 2.7218 | val_loss 2.7405\nEpoch 08 | train_loss 2.7068 | val_loss 2.7299\nEpoch 09 | train_loss 2.6942 | val_loss 2.7217\nEpoch 10 | train_loss 2.6817 | val_loss 2.7138\nEpoch 11 | train_loss 2.6713 | val_loss 2.7097\nEpoch 12 | train_loss 2.6622 | val_loss 2.7067\nEpoch 13 | train_loss 2.6537 | val_loss 2.7025\nEpoch 14 | train_loss 2.6456 | val_loss 2.6986\nEpoch 15 | train_loss 2.6382 | val_loss 2.6989\nEpoch 16 | train_loss 2.6323 | val_loss 2.6961\nEpoch 17 | train_loss 2.6253 | val_loss 2.6954\nEpoch 18 | train_loss 2.6198 | val_loss 2.6956\nEpoch 19 | train_loss 2.6146 | val_loss 2.6935\nEpoch 20 | train_loss 2.6097 | val_loss 2.6926\nEpoch 21 | train_loss 2.6050 | val_loss 2.6931\nEpoch 22 | train_loss 2.6003 | val_loss 2.6935\nReduced LR: 1.00e-03 -> 5.00e-04\nEpoch 23 | train_loss 2.5870 | val_loss 2.6879\nEpoch 24 | train_loss 2.5839 | val_loss 2.6867\nEpoch 25 | train_loss 2.5811 | val_loss 2.6896\nEpoch 26 | train_loss 2.5790 | val_loss 2.6888\nReduced LR: 5.00e-04 -> 2.50e-04\nEpoch 27 | train_loss 2.5721 | val_loss 2.6872\nReduced LR: 2.50e-04 -> 1.25e-04\nEarly stopping.\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# ====== SAMPLING ======\nimport numpy as np\nimport tensorflow as tf\n\n\nSTART_ID = START_ID  \nEND_ID   = END_ID  \nPAD_ID   = PAD_ID   \nV        = vocab_size\n\n@tf.function\ndef _forward_logits(inp_ids: tf.Tensor) -> tf.Tensor:\n    \"\"\"inp_ids: [B, L] int32  -> logits: [B, L, V]\"\"\"\n    return model(inp_ids, training=False)\n\ndef sample_password(max_len=20, temperature=0.9, forbid_ids=None):\n    \"\"\"\n    Start at <s>, sample one token at a time using temperature.\n    Stops at </s> or when max_len is reached. Never emits <PAD>.\n    Returns: list[int]  (token IDs without <s> and without </s>)\n    \"\"\"\n    if forbid_ids is None:\n        forbid_ids = {PAD_ID}\n\n    seq = [START_ID]\n    for _ in range(max_len):\n        inp = tf.constant([seq], dtype=tf.int32)     # [1, t]\n        logits = _forward_logits(inp)                # [1, t, V]\n        next_logits = logits[:, -1, :]               # [1, V]\n\n        # Mask forbidden ids (e.g., PAD) by setting very low logit\n        if forbid_ids:\n            mask_vec = np.zeros((V,), dtype=np.float32)\n            for i in forbid_ids:\n                if 0 <= i < V:\n                    mask_vec[i] = -1e9\n            next_logits = next_logits + tf.constant(mask_vec)[None, :]\n\n        # Temperature\n        next_logits = next_logits / float(temperature)\n        probs = tf.nn.softmax(next_logits, axis=-1)  # [1, V]\n\n        # Sample one id\n        next_id = tf.random.categorical(tf.math.log(probs), num_samples=1)[0, 0].numpy().item()\n\n        if next_id == END_ID:\n            break\n        seq.append(next_id)\n\n    # drop the initial <s>\n    return seq[1:]\n\ndef ids_to_string(ids):\n    return \"\".join(idx_to_ch[i] for i in ids)\n\ndef sample_many(n=10, max_len=20, temperature=0.9):\n    out = []\n    for _ in range(n):\n        ids = sample_password(max_len=max_len, temperature=temperature)\n        out.append(ids_to_string(ids))\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T19:15:26.279855Z","iopub.execute_input":"2025-08-31T19:15:26.280641Z","iopub.status.idle":"2025-08-31T19:15:26.290083Z","shell.execute_reply.started":"2025-08-31T19:15:26.280599Z","shell.execute_reply":"2025-08-31T19:15:26.289331Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Try multiple temperatures\nfor T in (0.7, 0.9, 1.2):\n    samples = sample_many(n=10, max_len=20, temperature=T)\n    print(f\"\\nT={T}\")\n    for s in samples:\n        print(s)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T19:16:13.248702Z","iopub.execute_input":"2025-08-31T19:16:13.249012Z","iopub.status.idle":"2025-08-31T19:16:16.649695Z","shell.execute_reply.started":"2025-08-31T19:16:13.248988Z","shell.execute_reply":"2025-08-31T19:16:16.649044Z"}},"outputs":[{"name":"stdout","text":"\nT=0.7\njuliana14\ntorycordish\nweebeer1\nmaboz1994\nlilthpazz\nkinguis\nyrnisme27\nvanica\nsexysharie23\nsam71680\n\nT=0.9\ndaida0803\nnrgoakkes\nzanyey\n0870363419\ndarkchristider\n51661500alf\n061207d\n112885\ndarket20\ncvinkdown\n\nT=1.2\nhondsest2\nakdakherv\nbianmeartahs\n2359w925\n0912710445\nmrs15x\ndinger1407\nlousendean\n_4E>FER\n3800351jg\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\n\n# Assumes you already have:\n# - model (loaded with best weights)\n# - START_ID, END_ID, PAD_ID\n# - ch_to_idx (char -> id), idx_to_ch (id -> char), vocabulary\nV = vocab_size  # for clarity\n\n@tf.function\ndef _forward_logits(inp_ids: tf.Tensor) -> tf.Tensor:\n    \"\"\"inp_ids: [B, L] int32 -> logits: [B, L, V]\"\"\"\n    return model(inp_ids, training=False)\n\ndef string_to_ids(s: str, ch_to_idx: dict) -> list[int]:\n    \"\"\"Map raw string to list of token ids; ignores chars not in vocab.\"\"\"\n    out = []\n    for ch in s:\n        if ch in ch_to_idx:\n            out.append(ch_to_idx[ch])\n        # else: skip unknowns (or map to a special <UNK> if you added one)\n    return out\n\ndef avg_logprob_per_char_from_ids(ids: list[int]) -> float:\n    \"\"\"\n    Teacher-forced log-prob per char (includes predicting </s>).\n    ids: just the password chars (no <s>, no </s>, no <PAD>).\n    \"\"\"\n    if len(ids) == 0:\n        return float(\"-inf\")\n\n    inp_seq = [START_ID] + ids               # <s> + chars\n    tgt_seq = ids + [END_ID]                 # chars + </s>\n\n    inp = tf.constant([inp_seq], dtype=tf.int32)     # [1, L]\n    tgt = tf.constant([tgt_seq], dtype=tf.int32)     # [1, L]\n\n    logits = _forward_logits(inp)                    # [1, L, V]\n    log_probs = tf.nn.log_softmax(logits, axis=-1)   # [1, L, V]\n\n    # gather log p(target_t | context)\n    # Build indices for gather_nd: [[0, t, tgt[t]] for each t]\n    batch_idx = tf.zeros_like(tgt, dtype=tf.int32)          # [1, L]\n    time_idx  = tf.range(tf.shape(tgt)[1], dtype=tf.int32)[None, :]  # [1, L]\n    idx = tf.stack([batch_idx, time_idx, tgt], axis=-1)     # [1, L, 3]\n\n    tok_logps = tf.gather_nd(log_probs, idx)                # [1, L]\n    return float(tf.reduce_mean(tok_logps).numpy())\n\ndef avg_logprob_per_char(password: str, ch_to_idx: dict) -> float:\n    ids = string_to_ids(password, ch_to_idx)\n    return avg_logprob_per_char_from_ids(ids)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T19:20:35.681112Z","iopub.execute_input":"2025-08-31T19:20:35.681431Z","iopub.status.idle":"2025-08-31T19:20:35.707612Z","shell.execute_reply.started":"2025-08-31T19:20:35.681409Z","shell.execute_reply":"2025-08-31T19:20:35.706845Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"weak = \"password123\"\nstrong = \"Aq9!fT7#Lm2\"   # or any random mixed string you pick\n\nprint(\"weak avg logp:\",   avg_logprob_per_char(weak, ch_to_idx))\nprint(\"strong avg logp:\", avg_logprob_per_char(strong, ch_to_idx))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-31T19:20:49.720723Z","iopub.execute_input":"2025-08-31T19:20:49.721284Z","iopub.status.idle":"2025-08-31T19:20:49.925875Z","shell.execute_reply.started":"2025-08-31T19:20:49.721258Z","shell.execute_reply":"2025-08-31T19:20:49.925225Z"}},"outputs":[{"name":"stdout","text":"weak avg logp: -1.2708632946014404\nstrong avg logp: -5.2147297859191895\n","output_type":"stream"}],"execution_count":26}]}